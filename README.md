
arxiv \| 南洋理工大学开源 基于文本线索实现复杂环境中的鲁棒闭环检测 【Robust Loop Closure by Textual Cues in Challenging Environments】 


文章链接：[\[2410\.15869] Robust Loop Closure by Textual Cues i...](http://arxiv.org/abs/2410.15869 "%5B2410.15869%5D%20Robust%20Loop%20Closure%20by%20Textual%20Cues%20i...")


开源仓库：[GitHub \- TongxingJin/TXTLCD: This repository is fo...](https://github.com/TongxingJin/TXTLCD "GitHub%20-%20TongxingJin%2FTXTLCD%3A%20This%20repository%20is%20fo...")  


 


 


## 在具有挑战性的环境中通过文本提示实现稳健的循环闭合


**摘要：**回环检测是机器人导航中的一项重要任务。然而，现有的方法大多依赖于环境的一些隐式或启发式特征，在走廊、隧道和仓库等常见环境中仍然无法工作。事实上，在这种无特征、退化和重复（FDR）的环境中导航即使对人类来说也会构成重大挑战，但周围环境中明确的文本提示通常会提供最好的帮助。这启发我们提出一种基于 FDR 环境中明确的人类可读文本提示的多模式闭环方法。具体来说，我们的方法首先基于光学字符识别（OCR）提取场景文本实体，然后基于精确的激光雷达里程计创建文本线索的本地地图，最后通过图论方案识别闭环事件。实验结果表明，该方法比仅依赖视觉和激光雷达传感器的现有方法具有更优越的性能。为了造福社区，我们在https://github.com/TongshingJin/TXTLCD发布了源代码和数据集。


索引术语——环路闭合、LiDAR SLAM、定位


### I. 简介


　　近年来，激光雷达惯性里程计（LIO）已成为移动机器人领域的支柱\[1]\-\[3]。值得注意的是，较新的 Livox Mid\-360 3D LiDAR 现在的成本与英特尔实感 D455 摄像头相似，提供更宽的视野、更远的范围和更高的精度。在连续定位方面，基于 LiDAR 的方法明显证明了比传统视觉 SLAM 更好的准确性和鲁棒性，并且消除了大多数应用中对视觉 SLAM 的需求，正如 Hilti SLAM 挑战 \[4] 所反映的那样。然而，基于LiDAR的闭环检测（LCD）方法，例如稳定三角描述符（STD）\[5]、扫描上下文（SC）\[6]和强度扫描上下文（ISC）\[7]，通常很难找到准确的在退化和重复的环境中匹配。虽然基于视觉的 LCD \[8]\-\[10] 提供了更大的特征描述符维度，但它们大多数对照明和视点变化敏感。


　　尽管具有密集的特征表示，但基于视觉的 LCD 故障仍然会发生。当处理具有计算限制的无特征、退化和重复 (FDR) 环境时，LCD 挑战就会出现。 FDR 环境中 LIO 的高效、简单且直观的 LCD 解决方案存在差距，能够反映类人流程。


　　本文中提出的研究从人类经常依赖其环境中的文本线索来确定其位置的观察中获得了灵感。事实上，这些文本提示的设计目的通常是帮助人类在 FDR 环境中导航（图 1），并且可以采用多种形式，例如寻路标志、铭牌和其他形式的基于语言的标牌。 TextSLAM \[11]、\[12] 是第一个将场景文本紧密集成到视觉 SLAM 管道中的方法，并在文本丰富的商业广场中展示了其有效性。与他们的方法相比，我们的方法进一步利用场景文本的空间结构来验证回环的真实性，同时在文本密度适中的环境中有效运行，这是大多数现实世界场景的典型。


[![](https://img2024.cnblogs.com/blog/2226321/202410/2226321-20241024162525135-1592370116.png)](https://github.com)


　图 1\. 常见的 FDR 场景示例，其中人类使用可读的文本符号及其空间排列自然地进行导航。


这启发我们使用文本提示来理解全球位置。


　　基于这一灵感，我们提出了一种多模态 (MM) 闭环解决方案，该解决方案利用 FDR 场景中的场景文本提示。具体来说，我们采用成熟的视觉光学字符识别（OCR）技术来检测当前位置附近存在的场景文本实体，然后基于低漂移​​LIO，创建本地文本实体映射（LTEM）来编码这些文本的特殊空间排列，可以用作验证候选闭环真实性的令牌。后端位姿图将引入闭环约束，以增强典型 FDR 环境中状态估计的鲁棒性和准确性。**我们的工作贡献可概括如下：**


　　1）我们通过融合激光雷达和视觉数据，引入了一种新颖的文本实体表示、估计和管理方法，该方法支持高效的闭环检索和对齐。 


　　2）我们提出了一种用于同一文本实体观察的关联方案，然后用于创建闭环并提高状态估计准确性。特别是，我们采用图论方法来识别候选闭环的真实性。 


　　3）我们将我们的方法与LiDAR里程计相结合，形成SLAM框架，并进行广泛的实验，以证明其与最先进的（SOTA）方法相比的竞争性能。 


　　4）我们发布源代码和调查分级的高精度数据集，以造福社区。


### 二.相关工作


　　用于全球定位的视觉和激光雷达融合是感知任务中的一个常见问题，该问题已在各种先前的研究中得到解决。


　　传统上，全局定位可以通过视觉里程计或SLAM方法来实现。然而，视觉方法在处理无特征区域、光照变化和远处物体时往往缺乏鲁棒性\[13]。通常，视觉因素必须辅以其他因素，例如 IMU 或 UWB \[14]，以提高效率和鲁棒性。最近，LiDAR 方法已成为前端里程计估计的主流，因为与大多数与视觉融合的实时方法相比，LIO \[1]、\[3] 始终能产生优异的结果。随着基于 LiDAR 的新型低成本解决方案的出现，基于视觉的方法在机器人定位中的受欢迎程度已经下降 \[4]。


　　实现全局定位的另一种方式是通过LCD。传统上，基于视觉的方法是主流，具有手工制作的特征。 DBOW2 \[8]使用基于BRIEF特征\[15]的二进制视觉词模型改进了实时LCD。近几十年来，基于学习的方法\[9]由于在处理视点和外观变化方面具有更好的性能而在LCD中占据主导地位。作为\[9]的扩展，\[10]中提出了一种基于最佳交通聚合的视觉地点识别模型，在许多基准上取得了SOTA结果。然而，由于有限的几何理解、FDR 环境和照明的变化，基于视觉的 LCD 仍然远非完美。


　　最近，基于激光雷达的LCD方法在现场机器人领域得到了广泛的探索，以实现精确的几何测量和照明不变性。 SC系列目前被认为是基于LiDAR的LCD最流行的方法\[6]，\[16]，其主要思想是采用投影和空间分区来编码整个点云，后来的工作通过集成对这一思想进行了改进强度\[7]和语义信息\[17]。然而，这一系列方法无法估计候选帧之间完整的 SE3 相对姿势，并且依赖里程计姿势来拒绝错误循环，从而使它们容易受到显着的里程计漂移的影响。 


　　STD \[5] 提出通过聚合局部点特征来创建基于三角形的描述符，使用每条边的长度作为哈希表中的键，通过投票方案找到闭环候选者。最近的工作被称为二元三角组合（BTC）\[18]，将 STD 与二元模式相结合，以提高速度和视点不变性。 BTC 目前处于抢先体验阶段，尚未可用于开源验证。然而，这些方法在 FDR 场景中遇到了困难，其中相似的空间形状、强度和语义可能会导致循环闭合中的歧义。


　　TextSLAM \[11]、\[12] 是第一个将场景文本紧密集成到基于点的视觉 SLAM 框架中的。它选择观察最可见文本对象的前十个历史关键帧作为循环闭合的候选者。共可见性要求需要多个文本对象在循环闭合框架中可见，这限制了其在现实环境中的适用性。为了克服这个限制，我们借助低漂移 LIO 创建本地文本实体图，并使用场景文本的空间排列检查候选闭环的真实性，从而使其在具有中等文本密度的更常见场景中有效。


### 三．方法


 　　在本节中，我们描述将场景文本表示为具有内容和姿势属性的文本实体并在 LiDAR 帧中观察它们的过程。然后，我们解释在文本观察之间创建关联关系并使用图论方法识别候选闭环真实性的原理。我们方法的工作流程如图2所示。


[![](https://img2024.cnblogs.com/blog/2226321/202410/2226321-20241024172150797-488388639.png)](https://github.com)


图 2\. 基于文本提示的循环闭合的pipeline。相机和激光雷达数据被融合以估计文本实体姿势并创建对场景文本的特定排列进行编码的本地文本实体地图。


应用一种新颖的图论方案来验证从在线数据库检索的候选闭环的真实性，并且每当新的闭环闭合时就执行位姿图优化，以减轻累积里程计漂移并确保全局地图的一致性。


　　符号：我们定义了四个主要坐标系：世界坐标系W、LiDAR坐标系L和相机坐标系C。我们使用TW Lt来表示LiDAR在世界坐标系中的SE3位姿在时间戳t。为了简单起见，我们可以省略世界坐标系的上标W，并将它们重写为TLt和TL。另外，后端位姿图优化采用LiDAR位姿{TLt}tn t\=t0作为节点。类似地，T L C 将用于表达相机和激光雷达之间的外在参数，TC文本和TL文本分别是在相机和激光雷达帧中表达的文本实体姿态。


[![](https://img2024.cnblogs.com/blog/2226321/202410/2226321-20241024171453647-943571268.png)](https://github.com)


#### A.文本实体观察


　　我们将场景文本抽象为包含两个属性的文本实体：文本内容和SE3位姿。文本内容是指可以通过OCR实现的文本字符串，而位姿观测是通过相机和激光雷达测量的融合来实现的。 


　　1）文本内容解读：OCR是一种成熟的技术，它首先以多边形的形式定位图像中的文本区域，然后将感兴趣的区域转换为可读的文本内容。在我们的实现中，我们采用 AttentionOCR \[19] 来提取场景文本，它提供置信度分数来帮助过滤掉不可靠的识别结果。 


　　2）文本实体表示：受TextSLAM \[11]、\[12]的启发，可以合理地假设场景文本实体通常位于平坦表面或局部平面上。例如，布告栏上的通知、房间号、消防设施上的铭牌以及紧急出口标志。如图3所示，我们定义场景文本区域左边缘的中点作为文本实体的原点。 x 轴指向文本右边缘的中点，z 轴与局部平面的法线方向对齐并指向相机，y 轴由右手定则确定。


[![](https://img2024.cnblogs.com/blog/2226321/202410/2226321-20241024171755154-1255192123.png)](https://github.com)


 图 3\. 文本实体表示的图示


 


　　3）姿态估计：为了估计相机帧中文本实体的 SE3 位姿，我们首先将过去一秒的 LiDAR 扫描累积到本地点云地图中并通过激光雷达和相机之间的外部参数将其投射相机框架中：


[![](https://img2024.cnblogs.com/blog/2226321/202410/2226321-20241024172554080-1602819404.png)](https://github.com)


　　其中pL是LiDAR坐标系中的LiDAR点坐标，TC L是LiDAR和相机坐标系之间的外参，pC是相机坐标系中的点坐标。然后，LiDAR点将进一步投影到图像坐标中：


[![](https://img2024.cnblogs.com/blog/2226321/202410/2226321-20241024172856475-8847600.png)](https://github.com)


 　其中K是相机的本征矩阵，\[u,v]⊤是LiDAR点所在的像素坐标。


　　由于场景文本通常附加到局部平面，因此可以通过 RANSAC 在文本所在区域内检测到的点集上估计相机帧中的平面参数。我们将相机框架中的平面表示为：


[![](https://img2024.cnblogs.com/blog/2226321/202410/2226321-20241024173029842-1979015270.png)](https://github.com)


　　其中n是平面的法线，p是平面上的任意点，d是相机光心到平面的距离。给定平面参数 (n, d) 和点 pC 的投影坐标 \[u, v]，可以如下恢复该点的深度：


 　   OCR 检测到的每个文本实体都带有一个边界框。我们将边界框左侧和右侧的中点分别表示为 pC l 和 prC 。我们选择 pC l 作为文本实体的位置，nx ≜ pC r −pC l ∥prC −pC l ∥ x 轴的单位向量。因此，文本实体的姿态矩阵定义为：


[![](https://img2024.cnblogs.com/blog/2226321/202410/2226321-20241024173205129-1145490795.png)](https://github.com)


　　由于相机和激光雷达是不同的模态传感器，并且在不同的时间点触发，因此文本实体将进一步锚定到图像时间戳 tj 之前时间戳 ti 的最新激光雷达帧中，其在激光雷达帧中的 SE3 位姿将表示为：


[![](https://img2024.cnblogs.com/blog/2226321/202410/2226321-20241024173236555-1088154927.png)](https://github.com):[悠兔机场](https://xinnongbo.com)


 　　其中 ti 和 tk 分别是图像时间戳 tj 之前和之后最近的两个 LiDAR 时间戳。 interpolate(T , s) 是恒等变换和 T 之间按因子 s ∈ (0, 1\) 进行的线性插值； T Li 文本是文本实体在其锚定 LiDAR 框架中的 SE3 位姿。为简单起见，今后我们将仅处理文本实体相对于 LiDAR 帧 T L 文本的姿态。


#### B.文本观察管理


　　为了支持有效的循环闭合存储、检索和对齐，我们将所有历史文本实体观察保存在文本实体观察数据库中，该数据库由文本字典和由哈希映射实现的框架字典组成（图1）。 2）。文本字典使用文本字符串作为键，所有观察文本内容的激光雷达帧的索引以及它们估计的文本实体姿势作为值，这使得能够快速检索观察特定文本内容的候选帧。帧字典利用帧索引作为键，使用该帧中所有观察到的文本实体的内容和估计姿势作为值，有助于创建候选帧附近的本地文本实体地图。


#### C. 基于文本实体的循环闭合检测和对齐


　　在各种环境中发现了多种场景文本，提供了对相关实体的功能和位置的见解。与二维码或其他地标不同，场景文本的优点是不需要专门部署，并且可以与人类导航无缝集成。我们将场景文本分为两类：ID 文本和通用文本，其中 ID 文本是类似地址的文本，可以帮助我们识别特定的房间或对象，通用文本是其他一切，例如退出、危险、电源。基于文本实体，我们应用不同的闭环检测策略。 


　　**1）ID文本**：ID文本是指遵循人类设计的特殊约定的文本，用于识别建筑物或地图内的特定对象。例如，S1\-B4c\-14表示S1栋，地下四层（地下一层），c区，14号房间，而S2\-B3c\-AHU3则表示S2栋，地下三层，c区，14号房间。空气处理单元3。可以根据应用环境的预定义模式挑选出此类文本。 


　　ID文本通常被设计成具有排他性，例如门号或设备号，因此在不同时间重复检测到相同的ID文本内容表明闭环的可能性很高。当前和候选闭环姿势 TLi 和 TLj 之间的相对姿势先验 ̄ T Li Lj 计算如下，


[![](https://img2024.cnblogs.com/blog/2226321/202410/2226321-20241024174131555-1024558843.png)](https://github.com)


　　然而，ID 文本也可以在不同位置有多个实例，例如，一个房间可能有多个具有相同编号的门。因此，我们使用 ICP 检查来排除这种情况下的错误循环候选。此外，ICP可以提供更精确的相对位姿先验（9），这有利于全局位姿图优化任务。


　　**2\) 通用文本：**一般来说，场景文本的很大一部分不指示专有位置信息，并且可以在场景内多次出现，例如，退出、禁止停车和停止。此类文本实体的关联可能是不明确的。为了解决这个问题，我们通过聚合当前位置附近的文本实体来创建本地文本实体地图（LTEM）。这样的 LTEM 对空间排列进行编码，并且可以用作当前姿势的标记，以验证候选循环与其他姿势的真实性，我们将在下面解释。


　　具体来说，LTEM 是一组由一组 LiDAR 里程计姿势观察到的所有文本实体，包括 ID 文本和通用文本。假设在当前姿态Tc（下标c代表当前），我们观察到一个文本实体Ec。我们将 Mc 定义为 LTEM，其中包含通过连续姿势 Tc \= {Tc−w 观察到的所有文本实体。 。 。 Tc}，其中 Tc−w 是距 Tc 一定距离 d 内的最早姿势。请注意，Mc 可能包含其他文本实体，其内容（即文本字符串）与 Ec 不同。


　　然后，我们使用 Ec 的内容从文本字典中搜索看到具有相同内容的文本实体的所有过去姿势。让我们将这些姿势的集合表示为 T 。对于每个候选先前姿势 Tp ∈ T ，我们将 Ep 表示为与 Tp 观察到的 Ec 具有相同内容的文本实体。然后，我们构建由连续姿势 Tp \= {Tp−w 观察到的所有文本实体的 LTEM。 。 。 Tp\+v}，其中 Tp−w 和 Tp\+v 分别是距 Tp 相同距离 d 内最早和最晚的姿势。我们将此 LTEM 表示为 Mp（图 4）。


　　给定 Mc 和 Mp，我们将构造一个关联关系集 A ≜ {ai, . 。 。 } \= {(Ec i ,Ep i ), . 。 。 }，其中 Ec i ∈ Mc，E p i ∈ Mp，并且 Ec i ，Ep i 具有相同的文本内容。集合 A 称为推定关联集合。显然，A可能由于一些重复的文本内容而包含不适当的关联。如图 4 所示，关联 a1、a2 和 a3 是互斥的，因为它们试图将来自 Mc 的相同文本实体与来自 Mp 的三个不同实体相关联。 


　　A 中这些假定关联之间的亲和关系可以用一致性图 G 表示，如图 5（a）所示。一致性图中的节点对应于图4中的假定关联，任意两个节点 ai 和 aj 之间的连接表示它们的兼容性，线条的深色进一步表示通过以下方式评估的几何一致性得分：


[![](https://img2024.cnblogs.com/blog/2226321/202410/2226321-20241024175343080-698996647.png)](https://github.com)


　　其中 pi 和 qi 是 ai 关联的两个文本实体的位置，pj 和 qj 是 aj 关联的文本实体的位置，∥·∥表示向量的欧几里德范数，s : R → \[0, 1] 是损失函数如果 x \> ε，则满足 s(0\) \= 1 且 s(x) \= 0，其中 ε 是阈值。这个分数表明一个 LTEM 中两个实体之间的距离应与另一个 LTEM 中对应实体之间的距离相匹配，因为 LTEM 内的 LiDAR 里程计漂移可以忽略不计。


 　　接下来，我们要找到一个全连接子图 G\* ⊂ G（图 5（b）），以及它们的节点子集 A\* ⊂ A，使得 A\* 中的任意一对关联 ai 和 aj 相互一致。这个问题是最大团问题的一个变体，CLIPPER \[20]将该问题表述为寻找最稠密的子图 G\*。在这项工作中，我们使用 CLIPPER 来解决这个问题。


　　一旦集合 A\* 被识别为至少具有三个元素并且 (Ec, Ep) ∈ A\*，两个实体 Ec 和 Ep 就可以用于构造相对位姿约束 ̄ Tp c 用于闭环，类似于 (9 ）。然后我们迭代 T 中的所有其他姿势以找到所有可能的闭环约束 ̄ Tp c 。算法1总结了一般过程。


　　我们注意到，在上面的多模态 LCD 和对齐方案中，LIO 输出与视觉检测信息紧密集成，首先是文本实体姿态的估计，其次是 LTEM 的构建。 LIO 短期导航的高精度对于我们方法的性能至关重要，而 VIO 无法实现，因为它的深度感知较差，定位漂移较大。


[![](https://img2024.cnblogs.com/blog/2226321/202410/2226321-20241024174527084-960751778.png)](https://github.com)


图 4\. 两个 LTEM 之间的假定关联。 LTEM Mc 和 Mp 包含一组分别由连续 LiDAR 位姿 Tc（绿色轨迹）和 Tp（蓝色轨迹）观察到的文本实体。具有相同文本内容的文本实体由相同颜色的球表示，通过紫色线连接以指示两个 LTEM 之间的假定关联。 Mc 中唯一的 EXIT 与 Mp 中的三个不同实体相关联。虽然关联 a1 是唯一正确的关联，但 a2 和 a3（由紫色虚线表示）应该被我们的图论闭环验证方法拒绝。


[![](https://img2024.cnblogs.com/blog/2226321/202410/2226321-20241024174650755-229159131.png)](https://github.com)


图 5\. 一致性图。线条的黑色表示连接的两个节点（假定的关联）之间的几何一致性。


[![](https://img2024.cnblogs.com/blog/2226321/202410/2226321-20241024175530102-1382323003.png)](https://github.com)


### 四．实验


　　在本节中，我们讨论数据集的开发以及与现有 SOTA 方法的比较。我们所有的实验都是在配备 Intel i7\-10875H CPU @ 2\.30GHz 和 NVIDIA GeForce RTX 2060 GPU 的笔记本电脑上进行的。我们实验的视频摘要可以在摘要中列出的项目页面上查看。


#### A. 数据集和实验设置


　　据我们所知，收集的数据集很少关注文本提示。我们注意到，在\[11]中，文本提示足够多，但是由于没有可用的激光雷达数据，我们无法制作精确的本地文本实体图。另一个关键要求是相机和激光雷达视场需要有足够的重叠。由于这些要求，我们发现没有公共数据集可用于基于文本的视觉激光雷达闭环研究。


　　为了填补这一空白，我们为重复和退化场景中的多模态 LCD 开发了高质量的数据集。我们的设置包括分辨率为 1920 × 1080 的相机、Livox Mid360 LiDAR 及其嵌入式 IMU。对于地面实况，我们使用 Leica MS60 扫描仪创建环境的高精度先验点云图，然后将 LiDAR 点云与这些先验地图配准以获得地面实况轨迹，类似于 \[4]、\[21]、\[ 22]。总共从 3 个不同的 FDR 场景收集了 8 个数据序列：室内走廊、半室外走廊和距离从 200 米到 500 米的跨楼层建筑物。它们的轨迹如图6所示。


[![](https://img2024.cnblogs.com/blog/2226321/202410/2226321-20241024180946914-335617396.png)](https://github.com)


图 6\.我们数据集中轨迹的图示。蓝线表示正常路径，而红线表示环路闭合事件的位置（第 IV\-B.1 节）。序列1、2、3和4、5、6在同一楼层捕获，而序列7和8则穿过不同楼层和垂直楼梯。在（a）、（b）、（c）、（d）中，我们在序列 1 和 4 中的不同走廊上展示了非常相似的场景。


　　我们将我们的方法与其他流行的开源 SOTA 作品进行比较，包括 SC \[6]、ISC \[7] 和 STD \[5]。为了保证实验的公平性，我们将FAST\-LIO2\[1]与不同的闭环方法集成，形成完整的SLAM系统进行评估。我们尝试保持所有参数不变，除了 ikdtree 图大小设置为 100m × 100m，分辨率为 0\.2m，扫描以体素分辨率 0\.1m 下采样。


　　虽然我们的方法旨在解决 FDR 场景中的 LiDAR 闭环问题，但我们还将数据集的图像序列输入 DBoW2 \[8] 和 SALAD \[10] 以评估其召回率和精度性能，因为我们的方法使用相机来检测文本。


#### B. LCD 召回和精度分析


　　1\) 真实闭环事件：根据地面真实姿势，我们将评估每个姿势以确定是否应该发生闭环检测。具体来说，考虑一个姿势 Tk，我们找到集合 Nk ≜ {Tp : ∥Tk ⊟ Tp∥ \< τ ∧ S(Tk, Tp) \> 10m, ∀p \< k}，其中 τ 是欧几里德距离阈值，我们将其设置为我们的实验中为 1\.0m 和 1\.7m，行进距离 S(Tk, Tp) ≜ Pk−1 i\=p ∥Ti\+1 ⊟ Ti∥。如果 Nk ̸\= ∅，则 Tk 被标记为闭环姿势。


　　对于每种闭环方法，我们评估其召回率和精确率。基于上述 Nk 的检查，该方法在位姿 Tk 处的预测可以是 TP、FP、TN 或 FN。因此，召回率是比率 P T P/(P T P \+ P F N )，准确率是比率 P T P/(P T P \+ P F P )。


 　2\) 召回：如表所示。 I，当τ\=1\.0m时，SALAD达到最好的召回性能，大多数情况下超过70%。我们的方法和 SC 都显示出有竞争力的结果，召回了 4 个序列中超过 50% 的循环。限制我们召回性能的主要因素是 OCR 模块的可重复性，这是指它能够在同一文本实体的多次观察中一致地检测到相同文本字符串结果。


　　ISC和STD的召回率最低，通常低于10%，因为它们设置了更严格的阈值来确认真正的闭环，因此与SC相比，准确率相对更高。


　　在环路闭合的情况下，较高的召回率可能并不表明两种方法之间具有决定性的优势，因为为每个重复走廊检测至少一个准确的环路足以显着减少里程计漂移。然而，精度更为关键，因为错误的闭环可能会破坏全局姿态估计和地图构建。在秒。 IVC，将检测到的循环纳入位姿图优化阶段时，可以通过有效减少绝对平移误差来验证召回的充分性。 


　　3）精度：如表1所示，当τ \= 1\.0m时，SALAD表现出了具有竞争力的性能，获得了5个第二名的成绩，而ISC则脱颖而出，成为最有效的纯激光雷达闭环方法，在4个序列中实现了80%以上的精度率。 DBoW 在序列 1 和序列 2（室内走廊）中表现良好，准确率超过 80%。然而，在序列 4 和 5 中，性能显着下降至 40% 左右。我们的 FDR 数据集对于 DBoW 来说确实具有挑战性，因为它错误地在相似位置（例如 (a) 和 (b)、(c) 和 ( d) 如图6所示，即使对于人类来说也是可能很难区分。


[![](https://img2024.cnblogs.com/blog/2226321/202410/2226321-20241024181426841-1195653955.png)](https://github.com)


 　　序列7和序列8的多层建筑是典型的重复场景，不同楼层的布局非常相似，如图6（c）和（d）所示。事实上，有些人可能还会发现，如果没有文本指示器的帮助，很难区分走廊。所有比较方法的一个显着缺点是它们容易产生灾难性的错误闭环。图7显示了序列8中几种方法生成的部分轨迹。ISC和STD都错误地将来自不同楼层的帧关联为闭环，导致它们的轨迹偏离真实值。然而，我们的方法仍然可以使用房间或设备编号作为文本提示来区分不同的楼层，避免形成错误循环的风险。


[![](https://img2024.cnblogs.com/blog/2226321/202410/2226321-20241024182329569-329200256.png)](https://github.com)


图 7\.STD 和 ISC 轨迹与地面真实情况显着偏离，绿色虚线箭头表示收敛方向。相比之下，我们的轨迹始终接近真实情况。


 


　　此外，与完全重叠的序列相比，所有 SOTA 方法的精度在部分重叠的序列中显着下降，即使它们是从相同的环境中收集的，即序列 3 与 1\-2 和序列 6 与 4\-5。这揭示了它们预测错误闭环的倾向，当轨迹重叠相对较低时，这种倾向变得更加明显。


　　相比之下，我们的方法始终实现最佳性能，并在所有序列上获得超过 95% 的高精度，这得益于我们的图论闭环识别方案，该方案有效地利用了文本实体的空间排列。性能不完美的原因是我们设置了一个严格的阈值 τ \= 1\.0m 来确定环路闭合的发生率，如下所示。在 IV\-B.1 中。如果阈值稍微放宽到1\.7m，我们的方法可以达到100%的准确率，同时保持相同的召回率，如表1所示。


#### C.位姿图优化误差评估


　　我们以FAST\-LIO2作为前端里程计，在检测回环的同时进行全局位姿图优化。 FDR 环境对视觉里程计或 ORB\-SLAM \[23] 等 SLAM 方法提出了重大挑战，因为许多图像是面向墙壁捕获的，几乎没有可连续提取或跟踪的特征。同时，视觉方法SALAD并不是为闭环设计的，不能直接输出相对位姿估计以供后续的全局位姿优化。因此，我们仅分析不同 LiDAR LCD 方法与 EVO 评估的 FASTLIO2 集成时的位姿误差 \[24]。


　　如表二所示，通过利用循环闭合的文本提示，我们的方法有效地最小化了所有数据集的里程计漂移，始终实现最低的平均翻译误差。相比之下，ISC 和 STD 经常报告错误的闭环，导致与里程计姿势相比平均误差更高。我们数据集中的主要挑战是其对称和重复的布局，如图 6 (a)\-(d) 所示。


[![](https://img2024.cnblogs.com/blog/2226321/202410/2226321-20241024181928909-290963596.png)](https://github.com)


　　尽管我们避免在 10m 行进距离内的相邻姿势之间形成闭环，如 IV\-B.1 中所述，但 SC 可以检索行进距离略大于此阈值的姿势之间的循环，并在之前引入非相邻姿势之间的相对姿势约束循环是闭合的，与其他方法相比，SC 的平均翻译误差更小。


　　除了平均翻译误差之外，图8还显示了不同方法在3个序列中的误差分布。很明显，我们的方法始终实现最低的误差。与其他方法相比，它是上限，因为我们的方法创建的所有循环都是真实的，并且不会在位姿图中引入错误的约束。此外，我们的定位误差的分布在不同序列中保持一致。


[![](https://img2024.cnblogs.com/blog/2226321/202410/2226321-20241024182149846-70809197.png)](https://github.com)


　　图 8\. 平均误差分布


#### D. 运行时分析


　　我们分别评估序列 1、4 和 7 中方法的不同阶段的时间成本。结果如表 III 所示，表明 OCR 是最耗时的部分。不过，将来可以用其他 OCR 方法替代。


[![](https://img2024.cnblogs.com/blog/2226321/202410/2226321-20241024182045936-1726514870.png)](https://github.com)


### V. 结论


　　为了填补 FDR 场景中现有导航方法的空白，我们提出了一种回环方案，该方案利用受人类导航启发的场景文本提示。我们的方法融合激光雷达和视觉信息来观察环境中的文本实体，并通过图论方案识别候选闭环的真实性。我们收集了 FDR 场景中的多个数据集，并进行了全面的比较实验，以证明我们的方法的竞争力。我们的开源代码和数据集将可供社区使用。


 


 


 


 


附：


#### 一、介绍一下什么是OCR技术？


视觉光学字符识别（Optical Character Recognition，简称OCR）技术是一种将不同质量的扫描图像转换成可编辑文本格式（如PDF、Word等）的软件技术。它能够识别和处理印刷体文字、手写文字以及场景文字。OCR技术广泛应用于文档扫描、票据识别、车牌识别、证件信息提取等领域。


OCR技术的基本流程包括以下几个步骤：


1\. \*\*图像预处理\*\*：包括去噪、二值化、倾斜校正等，目的是提高图像质量，使文字更易于识别。


2\. \*\*文字检测\*\*：定位图像中的文字区域，确定文字的位置和形状。


3\. \*\*字符分割\*\*：将检测到的文字区域分割成单个字符。


4\. \*\*字符识别\*\*：对分割后的字符进行识别，转换成对应的文本字符。


5\. \*\*后处理\*\*：包括校验、纠错等步骤，提高识别的准确性。


6\. \*\*输出\*\*：将识别结果输出为可编辑的文本格式。


随着深度学习技术的发展，OCR技术在识别准确率和处理速度上都有了显著提升。现在，许多OCR服务提供商都采用了基于深度学习的模型，如卷积神经网络（CNN）和循环神经网络（RNN），以提高对复杂场景和不同字体的识别能力。


如果你需要将纸质文档转换成电子文本，可以使用市面上的OCR软件，如Adobe Acrobat、ABBYY FineReader等，或者使用在线OCR服务。这些工具通常都提供了用户友好的界面，使得文档转换变得简单快捷。


#### 二、介绍一下什么是基于LiDAR的闭环检测（LCD）方法


基于LiDAR的闭环检测（Loop Closure Detection, LCD）是SLAM（Simultaneous Localization and Mapping，即同步定位与建图）系统中的一个关键环节，其目的是识别机器人在环境中的重复访问位置，以纠正由累积误差引起的地图和轨迹漂移问题。以下是一些基于LiDAR的闭环检测方法的介绍：


1\. \*\*OverlapNet\*\*： \- 由德国波恩大学Photogrammetry and Robotics Lab开源的OverlapNet是一种用于激光雷达SLAM中闭环检测的代码。 \- 无需先验位姿信息，使用深度神经网络直接估计两个激光雷达扫描的重叠率和相对偏航角。 \- 结合里程计信息和重叠率预测实现闭环的检测和修正。 \- 可以在挑战性环境下估计回环信息，并在不同数据集上具有良好的泛化性能。


2\. \*\*LCDNet\*\*： \- LCDNet是一个端到端的系统，旨在解决自动驾驶汽车和其他移动机器人中的闭环检测问题。 \- 通过深度学习和几何方法的结合，LCDNet能够有效地检测潜在的循环闭合并执行点云间的匹配。 \- 它包含一个深度神经网络架构，能高效处理点云数据，识别潜在的循环闭合场景，并包括一个快速的点云注册模块。


3\. \*\*Voxel\-SLAM\*\*： \- 香港大学提出的Voxel\-SLAM是一个完整、精确和通用的激光雷达\-惯性SLAM系统。 \- 充分利用短期、中期、长期和多地图数据关联，实现实时估计和高精度制图。 \- 闭环检测通过利用长期数据关联减轻漂移，并通过位姿图优化来校正累积误差。


4\. \*\*基于点云的闭环检测方法\*\*： \- 一种快速完整的基于点云闭环检测的激光SLAM系统，通过计算关键帧的2D直方图和局部地图patch来检测闭环。 \- 使用2D直方图的归一化互相关作为当前关键帧与地图中关键帧之间的相似性度量，快速且具有旋转不变性。


5\. \*\*基于多尺度点云特征变换器的LiDAR\-SLAM闭环检测\*\*： \- 提出了一种基于多尺度点云特征提取和变换器全局上下文建模的闭环检测方法。 \- 利用体素稀疏卷积获取不同分辨率下的原始点云特征，并利用Transformer网络建立不同分辨率下特征之间的上下文关系。


6\. \*\*基于强度信息的LiDAR SLAM和闭环检测\*\*： \- 提出了一种新颖的基于强度的LiDAR\-SLAM框架，强调了激光雷达强度信息在稀疏特征环境中的重要性。 \- 使用强度圆柱投影形状上下文描述符进行闭环检测，并采用双值循环候选验证策略。


这些方法展示了闭环检测在LiDAR SLAM中的多样性和深度学习技术的融合，以提高闭环检测的准确性和鲁棒性。随着研究的深入，这些方法在自动驾驶、机器人导航等领域的应用前景十分广阔。


 


